Here is the cleanest, simplest, exam-ready cheat sheet for your ML mid-term.
It covers all topics you listed, written as if you know nothing, and in a way that will help you solve paper questions tomorrow.

â¸»

ğŸ“˜ MACHINE LEARNING MID-TERM CHEAT SHEET

â¸»

ğŸ”¹ 1. Basics of ML

Machine Learning = Making computers learn patterns from data without being explicitly programmed.

Goal:
Use data â†’ learn a model â†’ make predictions.

â¸»

ğŸ”¹ 2. Introduction

ML types:
	1.	Supervised Learning â€“ has labels
	2.	Unsupervised Learning â€“ no labels
	3.	Reinforcement Learning â€“ learn by rewards

â¸»

ğŸ”¹ 3. Supervised Learning

Data includes input X and output Y.

Examples:
	â€¢	Predict house price
	â€¢	Classify email as spam or not

Two types:
	â€¢	Regression â†’ continuous output (price, temperature)
	â€¢	Classification â†’ discrete output (0/1, spam/not spam)

â¸»

ğŸ”¹ 4. Unsupervised Learning

Data has only input X, no labels Y.

Goal:
	â€¢	Find structure/patterns

Examples:
	â€¢	Clustering (K-means)
	â€¢	Dimensionality reduction (PCA)

â¸»

ğŸ”¹ 5. Reinforcement Learning

An agent learns by:
	â€¢	Taking actions
	â€¢	Receiving rewards/punishments
	â€¢	Improving policy over time

Example: Chess AI, Self-driving.

â¸»

ğŸ”¹ 6. Encoding for Categorical Data

Machines only understand numbers â†’ convert categories to numbers.

Two common methods:

1. One-Hot Encoding

Red â†’ [1,0,0]
Blue â†’ [0,1,0]
Green â†’ [0,0,1]

2. Label Encoding

Red â†’ 0
Blue â†’ 1
Green â†’ 2

â¸»

ğŸ”¹ 7. Models and Parameters

Model: A function that maps input â†’ output
Parameters: Values learned during training (weights, bias)

Example linear model:
y = w x + b
Here w and b are parameters.

â¸»

ğŸ”¹ 8. Loss Function

Loss = How wrong the model is on training data.

Examples:
	â€¢	MSE (Mean Squared Error) â†’ regression
	â€¢	Cross-entropy loss â†’ classification

â¸»

ğŸ”¹ 9. Risk of a Model

Two types:
	1.	Empirical Risk (Training Loss)
Loss calculated only on training data.
	2.	True Risk (Generalization Error)
How model performs on unseen data.

Goal: low true risk.

â¸»

ğŸ”¹ 10. Optimization

Process of finding the best parameters to minimize loss.

Most common: Gradient Descent

Steps:
	1.	Compute gradient
	2.	Update parameter:
\theta = \theta - \alpha \frac{dL}{d\theta}

â¸»

ğŸ”¹ 11. Machine Learning Pipeline

Typical pipeline:
	1.	Collect data
	2.	Clean data
	3.	Encode features
	4.	Split train/test
	5.	Train model
	6.	Evaluate model
	7.	Deploy

â¸»

ğŸ”¹ 12. Machine Learning for Prediction

We learn a function:
\hat{y} = f(x)
Goal: Predict output for new unseen input.

â¸»

ğŸ”¹ 13. Linear Function

A linear function looks like:
f(x) = w x + b

Can draw a straight line on a graph.

â¸»

ğŸ”¹ 14. Choice of Loss Function

Depends on problem:
	â€¢	Regression â†’ MSE
	â€¢	Classification â†’ Cross entropy
	â€¢	Outliers present â†’ MAE (L1 loss)

â¸»

ğŸ”¹ 15. Linear Regression Algorithm

Goal: find best line through data.

Model:
\hat{y} = w x + b

Loss: MSE
L = \frac{1}{n} \sum (y - \hat{y})^2

Optimization: Gradient descent.

â¸»

ğŸ”¹ 16. Alternative Loss Functions
	â€¢	MAE (L1 Loss): robust to outliers
	â€¢	Huber Loss: combination of MSE + MAE
	â€¢	Log-cosh

â¸»

ğŸ”¹ 17. Pearson Correlation

Measures linear relationship between two variables.

Range:
	â€¢	+1 â†’ perfect positive
	â€¢	0 â†’ no relation
	â€¢	-1 â†’ perfect negative

Formula:
r = \frac{cov(X,Y)}{\sigma_x \sigma_y}

â¸»

ğŸ”¹ 18. Quadratic Feature Map

Transforms input into non-linear features.

Example:
Original feature: x
Quadratic map: [x, x^2]

Allows linear models to fit curved patterns.

â¸»

ğŸ”¹ 19. Biasâ€“Variance Tradeoff

Bias = error from wrong assumptions (model too simple)
Variance = model too sensitive to noise (overfitting)

Goal: find balance.
	â€¢	High bias â†’ Underfitting
	â€¢	High variance â†’ Overfitting

â¸»

ğŸ”¹ 20. Training Data vs Test Data

Training data: used to learn model
Test data: used to evaluate performance (never used in training)

â¸»

ğŸ”¹ 21. Regularization

Used to reduce overfitting.

Types:
	â€¢	L1 Regularization (Lasso) â†’ makes weights sparse
	â€¢	L2 Regularization (Ridge) â†’ reduces weight size

Loss becomes:

L_{\text{reg}} = L + \lambda ||w||

â¸»

ğŸ”¹ 22. Cross Validation

Most common: k-fold cross validation

Steps:
	1.	Split data into k parts
	2.	Train on k-1 parts
	3.	Test on the left-out part
	4.	Repeat k times
	5.	Take average score

Purpose: better estimate of true performance.

â¸»

ğŸ”¹ 23. Logistic Regression

Used for binary classification.

Model:
p = \frac{1}{1 + e^{-wx+b}}

Loss used: Log loss / Cross entropy

Output: probability (0 to 1)

â¸»

ğŸ”¹ 24. Confusion Matrix

	Predicted +	Predicted -
Actual +	TP	FN
Actual -	FP	TN


â¸»

ğŸ”¹ 25. Accuracy, Precision, Recall

Accuracy
\frac{TP+TN}{Total}

Precision (How many predicted positives are correct)
\frac{TP}{TP+FP}

Recall (How many actual positives detected)
\frac{TP}{TP+FN}

â¸»

ğŸ”¹ 26. KNN Model (K-Nearest Neighbors)

Lazy learning model.

Steps:
	1.	Calculate distance to all points
	2.	Pick k nearest
	3.	Take majority vote (classification)
or average (regression)

Distance usually: Euclidean.

â¸»

ğŸ”¹ 27. Decision Trees

Tree-like model that splits data based on questions.

Example:
Is temperature > 25?
â†’ Yes â†’ Hot
â†’ No â†’ Cold

Advantages:
	â€¢	Easy to interpret
	â€¢	Handles non-linear data

â¸»

ğŸ”¹ 28. Impurity Function

Measures how â€œmixedâ€ a node is.

Common impurity measures:

1. Entropy

H = -\sum p \log_2(p)

Range:
	â€¢	0 â†’ pure
	â€¢	High â†’ mixed

2. Gini Impurity

G = 1 - \sum p^2

â¸»

ğŸ”¹ 29. Information Gain

Used in decision trees for choosing the best split.

Information\ Gain = Entropy(parent) - \text{Weighted Entropy(children)}

Higher IG = better split.

â¸»

â­ Your Exam Strategy

If short-questions come â†’ write definitions.
If numerical â†’ practice:
	â€¢	Confusion matrix
	â€¢	Accuracy/precision/recall
	â€¢	Entropy + Information Gain calculation
	â€¢	Pearson correlation
	â€¢	Linear regression prediction