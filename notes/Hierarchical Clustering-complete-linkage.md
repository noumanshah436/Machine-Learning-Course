Perfect ğŸ‘
Letâ€™s now do **Hierarchical Clustering using COMPLETE LINKAGE** on the **same data points**, step by step, very clearly.

---

## Same Data Points (2-D)

| Point | (x, y) |
| ----- | ------ |
| A     | (1, 1) |
| B     | (2, 1) |
| C     | (4, 3) |
| D     | (8, 8) |
| E     | (9, 8) |

---

## What is Complete Linkage? (Key Rule â­)

> **Distance between two clusters = maximum distance between any pair of points across the clusters**

Mathematically:
$$
d(C_i, C_j) = \max_{x \in C_i,; y \in C_j} d(x,y)
$$

This favors **compact, tight clusters**.

---

## Step 1: Pairwise Distance Matrix

Using Euclidean distance:

| Pair | Distance         |
| ---- | ---------------- |
| Aâ€“B  | âˆš1 = **1**       |
| Aâ€“C  | âˆš13 â‰ˆ **3.61**   |
| Aâ€“D  | âˆš98 â‰ˆ **9.90**   |
| Aâ€“E  | âˆš113 â‰ˆ **10.63** |
| Bâ€“C  | âˆš8 â‰ˆ **2.83**    |
| Bâ€“D  | âˆš85 â‰ˆ **9.22**   |
| Bâ€“E  | âˆš100 = **10**    |
| Câ€“D  | âˆš41 â‰ˆ **6.40**   |
| Câ€“E  | âˆš50 â‰ˆ **7.07**   |
| Dâ€“E  | âˆš1 = **1**       |

---

## Step 2: Start with Individual Clusters

Initial clusters:
[
{A}, {B}, {C}, {D}, {E}
]

---

## Step 3: First Merge (Smallest Distance)

Smallest distance = **1**

* Merge **A & B**
* Merge **D & E**

New clusters:

* **Câ‚ = {A, B}**
* **Câ‚‚ = {C}**
* **Câ‚ƒ = {D, E}**

---

## Step 4: Update Distances (Complete Linkage)

Now compute **maximum distance** between clusters.

---

### Distance: `{A,B}` â€“ `{C}`

Distances:

* Aâ€“C = 3.61
* Bâ€“C = 2.83

Complete linkage:
[
\max(3.61,;2.83) = \boxed{3.61}
]

---

### Distance: `{A,B}` â€“ `{D,E}`

Distances:

* Aâ€“D = 9.90
* Aâ€“E = 10.63
* Bâ€“D = 9.22
* Bâ€“E = 10

Complete linkage:
[
\max(9.90,;10.63,;9.22,;10) = \boxed{10.63}
]

---

### Distance: `{C}` â€“ `{D,E}`

Distances:

* Câ€“D = 6.40
* Câ€“E = 7.07

Complete linkage:
[
\max(6.40,;7.07) = \boxed{7.07}
]

---

## Step 5: Second Merge

Smallest updated distance = **3.61**

* Merge **{A,B} & {C}**

Clusters now:

* **Câ‚ = {A, B, C}**
* **Câ‚‚ = {D, E}**

---

## Step 6: Final Merge (Optional)

Compute distance between final clusters using **complete linkage**.

Distances:

* Aâ€“D = 9.90
* Aâ€“E = 10.63
* Bâ€“D = 9.22
* Bâ€“E = 10
* Câ€“D = 6.40
* Câ€“E = 7.07

Complete linkage:
[
\max = \boxed{10.63}
]

* Merge at distance **10.63**

---

## Dendrogram (Conceptual)

```
           â”Œâ”€â”€â”€â”€ A
       â”Œâ”€â”€â”€â”¤
       â”‚   â””â”€â”€â”€â”€ B
   â”Œâ”€â”€â”€â”¤
   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€ C
â”€â”€â”€â”¤
   â”‚       â”Œâ”€â”€â”€â”€ D
   â””â”€â”€â”€â”€â”€â”€â”€â”¤
           â””â”€â”€â”€â”€ E
```

(Cut height differs from single linkage)

---

## Cutting the Dendrogram (Important â­)

If we cut at distance **< 7**:

* **Cluster 1** â†’ {A, B, C}
* **Cluster 2** â†’ {D, E}

Same final clusters as single linkage here, **but merge heights are different**.

---

## Why Complete Linkage is Better Than Single (Intuition)

| Single Linkage    | Complete Linkage  |
| ----------------- | ----------------- |
| Uses min distance | Uses max distance |
| Causes chaining   | Avoids chaining   |
| Loose clusters    | Compact clusters  |

---

## One-Line Exam Answer â­

> In complete linkage hierarchical clustering, clusters are merged based on the maximum pairwise distance between their points, producing compact clusters and avoiding chaining.

---

If you want next:

* **Average linkage on same data**
* **Side-by-side table (single vs complete vs average)**
* **Wardâ€™s method with variance explanation**
* **When hierarchical beats K-Means (theory + example)**
